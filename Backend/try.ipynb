{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n",
      "     ---------------------------------------- 7.2/7.2 MB 5.8 MB/s eta 0:00:00\n",
      "Collecting requests\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.1-cp39-cp39-win_amd64.whl (263 kB)\n",
      "     -------------------------------------- 263.9/263.9 KB 1.5 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "     -------------------------------------- 236.8/236.8 KB 2.9 MB/s eta 0:00:00\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp39-cp39-win_amd64.whl (151 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in i:\\dan dev\\nhce\\mini project\\image captioning\\imagecaption\\lib\\site-packages (from transformers) (23.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.6.3-cp39-cp39-win_amd64.whl (268 kB)\n",
      "     -------------------------------------- 268.1/268.1 KB 1.8 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.17\n",
      "  Using cached numpy-1.24.3-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in i:\\dan dev\\nhce\\mini project\\image captioning\\imagecaption\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "     -------------------------------------- 163.8/163.8 KB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in i:\\dan dev\\nhce\\mini project\\image captioning\\imagecaption\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
      "     -------------------------------------- 123.6/123.6 KB 3.7 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.1.0-cp39-cp39-win_amd64.whl (97 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "Installing collected packages: tokenizers, safetensors, urllib3, tqdm, regex, pyyaml, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, transformers\n",
      "Successfully installed certifi-2023.5.7 charset-normalizer-3.1.0 filelock-3.12.2 fsspec-2023.6.0 huggingface-hub-0.15.1 idna-3.4 numpy-1.24.3 pyyaml-6.0 regex-2023.6.3 requests-2.31.0 safetensors-0.3.1 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.30.1 urllib3-2.0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'I:\\Dan dev\\NHCE\\Mini Project\\Image Captioning\\imagecaption\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'i:\\Dan dev\\NHCE\\Mini Project\\Image Captioning\\imagecaption\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached torch-2.0.1-cp39-cp39-win_amd64.whl (172.4 MB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: filelock in i:\\dan dev\\nhce\\mini project\\image captioning\\imagecaption\\lib\\site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in i:\\dan dev\\nhce\\mini project\\image captioning\\imagecaption\\lib\\site-packages (from torch) (4.6.3)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.3-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, jinja2, torch\n",
      "Successfully installed MarkupSafe-2.1.3 jinja2-3.1.2 mpmath-1.3.0 networkx-3.1 sympy-1.12 torch-2.0.1\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow\n",
      "  Using cached Pillow-9.5.0-cp39-cp39-win_amd64.whl (2.5 MB)\n",
      "Installing collected packages: pillow\n",
      "Successfully installed pillow-9.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'i:\\Dan dev\\NHCE\\Mini Project\\Image Captioning\\imagecaption\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Dan dev\\NHCE\\Mini Project\\Image Captioning\\imagecaption\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at nlpconnect/vit-gpt2-image-captioning were not used when initializing VisionEncoderDecoderModel: ['decoder.transformer.h.0.attn.masked_bias', 'decoder.transformer.h.8.attn.masked_bias', 'decoder.transformer.h.10.attn.bias', 'decoder.transformer.h.0.crossattention.bias', 'decoder.transformer.h.8.crossattention.bias', 'decoder.transformer.h.9.attn.masked_bias', 'decoder.transformer.h.6.crossattention.bias', 'decoder.transformer.h.6.attn.bias', 'decoder.transformer.h.11.attn.masked_bias', 'decoder.transformer.h.2.attn.bias', 'decoder.transformer.h.4.crossattention.bias', 'decoder.transformer.h.6.crossattention.masked_bias', 'decoder.transformer.h.2.attn.masked_bias', 'decoder.transformer.h.2.crossattention.bias', 'decoder.transformer.h.10.crossattention.masked_bias', 'decoder.transformer.h.7.crossattention.bias', 'decoder.transformer.h.1.crossattention.masked_bias', 'decoder.transformer.h.3.attn.masked_bias', 'decoder.transformer.h.5.crossattention.masked_bias', 'decoder.transformer.h.0.crossattention.masked_bias', 'decoder.transformer.h.10.crossattention.bias', 'decoder.transformer.h.8.crossattention.masked_bias', 'decoder.transformer.h.1.attn.bias', 'decoder.transformer.h.2.crossattention.masked_bias', 'decoder.transformer.h.5.attn.bias', 'decoder.transformer.h.4.attn.masked_bias', 'decoder.transformer.h.1.attn.masked_bias', 'decoder.transformer.h.11.attn.bias', 'decoder.transformer.h.0.attn.bias', 'decoder.transformer.h.7.attn.bias', 'decoder.transformer.h.1.crossattention.bias', 'decoder.transformer.h.7.crossattention.masked_bias', 'decoder.transformer.h.10.attn.masked_bias', 'decoder.transformer.h.11.crossattention.masked_bias', 'decoder.transformer.h.5.attn.masked_bias', 'decoder.transformer.h.3.crossattention.masked_bias', 'decoder.transformer.h.9.crossattention.masked_bias', 'decoder.transformer.h.7.attn.masked_bias', 'decoder.transformer.h.4.attn.bias', 'decoder.transformer.h.6.attn.masked_bias', 'decoder.transformer.h.11.crossattention.bias', 'decoder.transformer.h.9.crossattention.bias', 'decoder.transformer.h.8.attn.bias', 'decoder.transformer.h.3.attn.bias', 'decoder.transformer.h.4.crossattention.masked_bias', 'decoder.transformer.h.3.crossattention.bias', 'decoder.transformer.h.9.attn.bias', 'decoder.transformer.h.5.crossattention.bias']\n",
      "- This IS expected if you are initializing VisionEncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisionEncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "i:\\Dan dev\\NHCE\\Mini Project\\Image Captioning\\imagecaption\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incorrect path or url, URLs must start with `http://` or `https://`, and 12.jpg is not a valid path",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m      3\u001b[0m image_to_text \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39mimage-to-text\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnlpconnect/vit-gpt2-image-captioning\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m image_to_text(\u001b[39m\"\u001b[39;49m\u001b[39m12.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mi:\\Dan dev\\NHCE\\Mini Project\\Image Captioning\\imagecaption\\lib\\site-packages\\transformers\\pipelines\\image_to_text.py:106\u001b[0m, in \u001b[0;36mImageToTextPipeline.__call__\u001b[1;34m(self, images, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images: Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mImage.Image\u001b[39m\u001b[39m\"\u001b[39m, List[\u001b[39m\"\u001b[39m\u001b[39mImage.Image\u001b[39m\u001b[39m\"\u001b[39m]], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     82\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39m    Assign labels to the image(s) passed as inputs.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m        - **generated_text** (`str`) -- The generated text.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mi:\\Dan dev\\NHCE\\Mini Project\\Image Captioning\\imagecaption\\lib\\site-packages\\transformers\\pipelines\\base.py:1120\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[0;32m   1113\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[0;32m   1114\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         )\n\u001b[0;32m   1118\u001b[0m     )\n\u001b[0;32m   1119\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mi:\\Dan dev\\NHCE\\Mini Project\\Image Captioning\\imagecaption\\lib\\site-packages\\transformers\\pipelines\\base.py:1126\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m-> 1126\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m   1127\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1128\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n",
      "File \u001b[1;32mi:\\Dan dev\\NHCE\\Mini Project\\Image Captioning\\imagecaption\\lib\\site-packages\\transformers\\pipelines\\image_to_text.py:109\u001b[0m, in \u001b[0;36mImageToTextPipeline.preprocess\u001b[1;34m(self, image, prompt)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess\u001b[39m(\u001b[39mself\u001b[39m, image, prompt\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 109\u001b[0m     image \u001b[39m=\u001b[39m load_image(image)\n\u001b[0;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m prompt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mi:\\Dan dev\\NHCE\\Mini Project\\Image Captioning\\imagecaption\\lib\\site-packages\\transformers\\image_utils.py:276\u001b[0m, in \u001b[0;36mload_image\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m    274\u001b[0m         image \u001b[39m=\u001b[39m PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mopen(image)\n\u001b[0;32m    275\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncorrect path or url, URLs must start with `http://` or `https://`, and \u001b[39m\u001b[39m{\u001b[39;00mimage\u001b[39m}\u001b[39;00m\u001b[39m is not a valid path\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         )\n\u001b[0;32m    279\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(image, PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mImage):\n\u001b[0;32m    280\u001b[0m     image \u001b[39m=\u001b[39m image\n",
      "\u001b[1;31mValueError\u001b[0m: Incorrect path or url, URLs must start with `http://` or `https://`, and 12.jpg is not a valid path"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "image_to_text = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "image_to_text(\"12.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Dan dev\\NHCE\\Mini Project\\Image Captioning\\imagecaption\\lib\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "val = image_to_text(\"Screenshot_33.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a painting of a cartoon character with a fire hydrant '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val[0]['generated_text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caption",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
